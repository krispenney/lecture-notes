<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/github-markdown-css/2.10.0/github-markdown.min.css" />
<div class="markdown-body">
<h2>MapReduce Algorithm Design</h2>
<h3>Common Theme</h3>
<p>Parallelization challenges come from (sharing state):</p>
<ul>
<li>Need to communicate partial results</li>
<li>Need to access shared resources</li>
<li><strong>Layer of abstraction</strong>: The datacenter is the computer
<ul>
<li>Allow the developer to avoid system level details.</li>
<li>dev specifies the computation that needs to be performed</li>
<li>framework handles the actual execution</li>
</ul>
</li>
<li>Scale out, not up</li>
<li>assume that components will break</li>
<li>move processing to the data, code is much smaller</li>
<li>process data in sequential order, avoid random access (which is expensive)</li>
</ul>
<h3>MapReduce Runtime</h3>
<ul>
<li>handle scheduling: assign workers to map and reduce tasks</li>
<li>move processes to data
<ul>
<li>Start up worker on nodes that hold the data.</li>
</ul>
</li>
<li>handles synchronization</li>
<li>handling errors and faults</li>
<li>gotchas
<ul>
<li>avoid object creation (expensive)</li>
<li>get data to maps and reducers
<ul>
<li>via job config params</li>
<li>Side data: distributed cache, read from HDFS in setup.</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3>Business Intelligence</h3>
<p>Organizations should retain data resulting from carrying out it's mission. Exploit those insights to benefit the organization.</p>
<h4>Virtuous product cycle: Generate revenue</h4>
<ol>
<li>a useful service</li>
<li>analyze user behaviour to extract insights (data science)</li>
<li>transform insights into actions (data products)</li>
<li>feedback</li>
</ol>
<h3>Cloud Computing</h3>
<ul>
<li>Utility computing: computing resources as a metered service, pay as you go</li>
<li>cloud makes it easier to start companies that generate big data, avoid ownership of the data</li>
<li>everything as a service</li>
</ul>
<h3>Namenode Responsibilities</h3>
<ul>
<li>managing the file system namespace</li>
<li>coordinating file operations</li>
<li>maintaining overall health</li>
</ul>
<p>files are divided into many splits, RecordReaders act as a cursor, passed to the Mappers.</p>
<ul>
<li>the splits can be arbitrary, down the middle of a record</li>
<li>RecordReaders always start reading from a complete block, may keep reading over the edge of a split to capture the last record.</li>
</ul>
<h3>Distributed GroupBy in MapReduce</h3>
<h4>Map side</h4>
<ul>
<li>map outputs are in memory in a circular buffer</li>
<li>when buffer reaches threshold, contents spilled to disk</li>
<li>spills are merged into a single partitioned file (sorted by partition)</li>
<li>combiners run during merges</li>
</ul>
<h4>Reduce Side</h4>
<ul>
<li>map outputs are copied over to reducers</li>
<li>sort is a multi-pass merge of map outputs (in memory, on disk)</li>
<li>combiner runs during the merges</li>
<li>final merge pass goes directly into reducer</li>
</ul>
<h4>Design Patterns</h4>
<ul>
<li>all algs must be expressed in m, r, c, p</li>
<li>no idea when things run, what order, which input a worker is processing</li>
<li>avoid: object creation, buffering</li>
<li>local aggregation: synchronization kills communication, kills performance, reduce the number of intermediate pairs that need to be processed.
<ul>
<li>in-mapper combining: fold combiner into the mapper, preserve state across multiple map calls.
<ul>
<li>pros: speed</li>
<li>cons: need to explicitly manage memory, order-dependent bugs</li>
</ul>
</li>
</ul>
</li>
<li>combiners are optional optimizations, may br run 0, 1, or multiple times
<ul>
<li>should not impact correctness</li>
</ul>
</li>
</ul>
<h5>Pairs vs. Stripes</h5>
<ul>
<li><strong>Pairs:</strong> Utilize the key as a pair, secondary sorting
<ul>
<li>pros: easy to implement + understand</li>
<li>cons: lots of intermediate pairs, combiners dont really work</li>
<li>may need custom sort order, have certain pairs show-up first <code>(a, *)</code>
<ul>
<li>pull values as part of the key for proper sorting</li>
</ul>
</li>
</ul>
</li>
<li><strong>Stripes:</strong> Group pairs into associative array
<ul>
<li>pros: less sorting and shuffling, can use combiners (element-wise operations)</li>
<li>cons: harder to implement, more complicated object, data structure manipulations,</li>
</ul>
</li>
</ul>
<p>tradeoffs:</p>
<ul>
<li>developer code vs. framework (sorting, grouping)</li>
<li>number of kv pairs</li>
<li>size + complexity of each kv pair: de/serialization overhead</li>
</ul>
<table>
<thead>
<tr>
<th>Pairs</th>
<th>Stripes</th>
</tr>
</thead>
<tbody>
<tr>
<td>more kv pairs</td>
<td>less</td>
</tr>
<tr>
<td>less combining</td>
<td>more combining</td>
</tr>
<tr>
<td>more sorting</td>
<td>less sorting + shuffling</td>
</tr>
<tr>
<td>simple reduce aggs</td>
<td>complex (slower) aggs</td>
</tr>
</tbody>
</table>
<h3>Pig</h3>
<ul>
<li>write in a higher level language, run a series of MapReduce jobs.</li>
</ul>
<p>Common model:</p>
<pre><code class="language-pig">LOAD # load from HDFS
FOREACH ... GENERATE # per tuple processing
FILTER # discard unwanted tuples: (map)
GROUP / COGROUP # group tuples
join # relational join (reduce)
STORE # write back to HDFS
</code></pre>
<ul>
<li>extend PIG via user defined functions (UDFs) in any language</li>
</ul>
<h3>Problems with MapReduce</h3>
<ul>
<li>always have to go back to HDFS</li>
<li>slow</li>
<li>Dryad: Graph processing framework
<ul>
<li>abstractions for vertex-to-vertex communication</li>
</ul>
</li>
</ul>
<h2>Spark</h2>
<ul>
<li>
<p>based on <strong>Resilient Distributed Datasets (RDD)</strong></p>
<ul>
<li>immutable, partitioned</li>
<li>perform transformations, lazy</li>
<li>actions, trigger the execution</li>
</ul>
</li>
<li>
<p>RDDs don't have to be written back to HDFS, can chain operations</p>
</li>
<li>
<p>fault tolerance: RDDs can be regenerated from the operations</p>
</li>
<li>
<p>why does it work</p>
<ul>
<li>associativity: group operations in any way: <code>1 + 3 + 2 = (1 + 3) + 2</code></li>
<li>commutativity: swap order of operands however you want: <code>(2 + 1) + 3 = 3 + (2 + 1)</code></li>
</ul>
</li>
</ul>
<h3>Monoids</h3>
<ul>
<li>semigroup: (M, operator)
<ul>
<li><img src="data/final_notes/latex-34fca2d8-393d-4166-867f-83cd26873e22.png" alt="latex-34fca2d8-393d-4166-867f-83cd26873e22"></li>
</ul>
</li>
<li>monoid: Semigroup + identity operation
<ul>
<li><img src="data/final_notes/latex-559b83d5-42d9-4e59-9a62-ea31038db25e.png" alt="latex-559b83d5-42d9-4e59-9a62-ea31038db25e"></li>
</ul>
</li>
<li>commutative Monoid: monoid + commutativity
<ul>
<li><img src="data/final_notes/latex-7f472451-acdb-4b90-8c52-10c265ed56c8.png" alt="latex-7f472451-acdb-4b90-8c52-10c265ed56c8"></li>
</ul>
</li>
<li>when you can't utilize monoids: sequence computations by sorting</li>
</ul>
<h2>Analyzing Text</h2>
<ul>
<li>Language Models: <img src="data/final_notes/latex-735078f5-097a-4032-bfcd-b4f828c2c217.png" alt="latex-735078f5-097a-4032-bfcd-b4f828c2c217">_</li>
<li>Use Markov Assumption to limit history to a fixed number of words: <img src="data/final_notes/latex-ab22e185-09bc-4ed0-a296-150ea2b88457.png" alt="latex-ab22e185-09bc-4ed0-a296-150ea2b88457"></li>
<li><img src="data/final_notes/latex-1c46113f-0e25-49a0-99c4-925355ec2220.png" alt="latex-1c46113f-0e25-49a0-99c4-925355ec2220"> unigram language model, <img src="data/final_notes/latex-9a0eb6c6-9a85-42c2-98d5-15b0f2211143.png" alt="latex-9a0eb6c6-9a85-42c2-98d5-15b0f2211143">: bigrams</li>
<li>Compute <strong>Maximum likelihood estimates (MLE)</strong> (count + divide)
<ul>
<li><img src="data/final_notes/latex-709ae8fd-92cf-4398-bab8-b7e30afbaaee.png" alt="latex-709ae8fd-92cf-4398-bab8-b7e30afbaaee"></li>
</ul>
</li>
<li><strong>Smoothing</strong>: Want to avoid zero probabilities
<ul>
<li>Laplace: add 1 to all counts</li>
<li>Jelinek-Mercer smoothing: weighted linear combination of lower-order models.</li>
<li>Kneser-Ney: discounted model with special continuouation n-gram model. Number of different contexts <img src="data/final_notes/latex-5fb45f19-fd3f-4dc7-98e1-a73eda02a714.png" alt="latex-5fb45f19-fd3f-4dc7-98e1-a73eda02a714"> has appeared in.</li>
<li>Stupid backoff: use the higher order language model if greater than zero, otherwise fallback to lower order models.
<ul>
<li>Solve the problem by throwing lots of data at it.</li>
</ul>
</li>
</ul>
</li>
<li>Bayes rule: <img src="data/final_notes/latex-ed93c9b6-523f-417d-8ff5-3c3e5fdcb8e1.png" alt="latex-ed93c9b6-523f-417d-8ff5-3c3e5fdcb8e1"></li>
</ul>
<h2>Document Retreival</h2>
<ul>
<li><strong>ranked retrieval</strong>: Order documents by how likely they are to be relevant.</li>
<li>Partitioning: Scalability</li>
<li>Replication: Redundancy</li>
<li>Caching: Speed</li>
<li>Routing: Load Balancing</li>
</ul>
<h3>TF.IDF Term Weighting</h3>
<ul>
<li>term weights consist of 2 parts: document and collection</li>
<li>high weights for terms that appear many times in a document, terms that appear in many documents should get low weights.</li>
</ul>
<p><img src="data/final_notes/latex-8f87cc09-bf35-4ec2-a516-6a58bfd21a0a.png" alt="latex-8f87cc09-bf35-4ec2-a516-6a58bfd21a0a"></p>
<h2>Graphs</h2>
<ul>
<li>hard: irregular structure, irregular access patterns</li>
<li>typical alg: local computations at each node, propogate results along the edges</li>
</ul>
<p>Representations:</p>
<ul>
<li><strong>Adjacency Matrix:</strong> nxn square matrix, n=number of verticies, <img src="data/final_notes/latex-b5be0b31-ae4d-4e68-b545-25b4741a1327.png" alt="latex-b5be0b31-ae4d-4e68-b545-25b4741a1327">_ iff there is an edge from i to j.
<ul>
<li>intuitive to iterate over rows and columns</li>
<li>lots of wasted space, for sparse graphs. easy to write, hard to run computations</li>
</ul>
</li>
<li><strong>Adjacency List:</strong>
<ul>
<li>Less wasted space, possible to compress. Easy to compute using outlinks (just iterate over the list foreach edge)</li>
<li>hard to compute over inlinks</li>
</ul>
</li>
<li><strong>Edge List:</strong> Explicitly enumerate all of the edge pairs
<ul>
<li>easy to add new edges, just add a pair to the list</li>
<li>waste alot of space.</li>
</ul>
</li>
</ul>
<p>Store undirected graphs, do one of:</p>
<ol>
<li>store both edges, make sure the algorithm de-dups</li>
<li>store as one edge, alg needs to recognize it goes in both directions.</li>
</ol>
<p>Manipulations</p>
<ul>
<li>Invert: <code>flatMap</code> and <code>regroup</code></li>
<li>Adjacency list -&gt; edge list: <code>flatMap</code> over adjacency list and emit tuples</li>
<li>Edge list -&gt; adjacency list: <code>groupby</code></li>
</ul>
<p>Single Source Shortest Path</p>
<ul>
<li>Recall: <strong>Dijkstra</strong>, start at source node, expand the field until the destination is reached.</li>
<li>Define inductivly
<ul>
<li><code>DistanceTo(s) = 0</code> b is reachable from a if b is on adjacency list of a</li>
<li><code>DistanceTo(p) = 1</code> For all nodes p reachable from s</li>
<li><code>DistanceTo(n) = 1 + min(DistanceTo(m), for m in M)</code>: For all nodes n reachable from some other set of nodes M</li>
</ul>
</li>
</ul>
<p>Parallel BFS</p>
<ul>
<li>For all nodes except the start, <img src="data/final_notes/latex-c30be04d-6257-4514-b73f-e5a564abcaf2.png" alt="latex-c30be04d-6257-4514-b73f-e5a564abcaf2"></li>
<li><strong>Mapper</strong>: For each m in adjacency list, emit <code>(m, d+1)</code>, emit the distance to yourself</li>
<li><strong>Sort/Shuffle</strong>: group distances by reachable nodes</li>
<li><strong>Reducer</strong>: Select the minimum distance path for each reachable node, additional book-keeping needed to keep track of the actual path.</li>
<li>Multiple Iterations: each MapReduce job expands the frontier by one step</li>
</ul>
<pre><code class="language-scala">class Mapper {
  def map(id: Long, n: Node) = {
    emit(id, n)
    val d = n.distance
    emit(id, d)
    for (m &lt;- n.adjacenyList) {
      emit(m, d+1)
    }
  }
}

class Reducer {
  def reduce(id: Long, objects: Iterable[Object]) = {
    var min = infinity
    var n = null
    for (d &lt;- objects) {
      if (isNode(d)) n = d
      else if (d &lt; min) min = d
    }
    n.distance = min
    emit(id, n)
  }
}
</code></pre>
<ul>
<li>run job, check for convergence, try again</li>
<li>MapReduce explores all possible paths in parallel, lots of wasted space, we only do useful work at the frontier.</li>
</ul>
<p>Single Source with weighted edges</p>
<ul>
<li>add weight w for each edge in adjacency list</li>
<li>in mapper emit <code>(m, d+w)</code> instead of <code>(m, d+1)</code></li>
<li>need to go through the entire graph</li>
</ul>
<p>Multiple Source shortest Path</p>
<ul>
<li>have an array of sources</li>
<li>Mapper emits an array of distances wrt each source</li>
<li>Reducer finds the minimum for each element in the array</li>
</ul>
<p>Generic Recipie:</p>
<ul>
<li>represent graph as adjacency list</li>
<li>perform local computations in the mapper</li>
<li>pass along partial results via outlinks, use the desination node as the key</li>
<li>perform aggregation in the reducer on inlinks to a node</li>
<li>iterate until convergence, external driver</li>
<li>pass the graph structure between iterations</li>
</ul>
<p>PageRank</p>
<ul>
<li>
<p>Based off of random walks around the web</p>
</li>
<li>
<p>Given page x with n inlinks</p>
<ul>
<li><img src="data/final_notes/latex-80600f86-fce6-4c8d-94e6-99a0af0bfd6b.png" alt="latex-80600f86-fce6-4c8d-94e6-99a0af0bfd6b"> is out-degree of t</li>
<li><img src="data/final_notes/latex-999919b0-264d-4296-bbb3-e992713571a7.png" alt="latex-999919b0-264d-4296-bbb3-e992713571a7"> is the prob of a random jump</li>
<li><img src="data/final_notes/latex-08f2bd90-1ffc-468f-a9ca-28f8a2a63255.png" alt="latex-08f2bd90-1ffc-468f-a9ca-28f8a2a63255"> total number of nodes in the graph</li>
<li><img src="data/final_notes/latex-7b5fd9a7-d48d-47a7-b738-56920e53c093.png" alt="latex-7b5fd9a7-d48d-47a7-b738-56920e53c093"></li>
<li>Second pass for dangling nodes: <img src="data/final_notes/latex-7291d964-2b89-489c-8f11-895ded90a938.png" alt="latex-7291d964-2b89-489c-8f11-895ded90a938">
<ul>
<li><img src="data/final_notes/latex-30731539-3557-4c08-9e17-358465252e8a.png" alt="latex-30731539-3557-4c08-9e17-358465252e8a"> is previous pagerank mass</li>
<li><img src="data/final_notes/latex-779440ed-8801-4793-8487-865b81ea65ce.png" alt="latex-779440ed-8801-4793-8487-865b81ea65ce"> is the missing PageRank mass</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Convergence criteria</p>
<ul>
<li>iterate until PageRank values don't change</li>
<li>Iterate until PageRank rankings don't change</li>
<li>Fixed number of iterations</li>
</ul>
</li>
<li>
<p>Use log probabilities to avoid small values</p>
</li>
<li>
<p>weighted PageRank</p>
</li>
<li>
<p>Personalized PageRank</p>
</li>
<li>
<p>Spark: <code>join</code>, <code>flatMap</code>, <code>reduceByKey</code></p>
<ul>
<li>cache the adjacency list</li>
<li>Join with PageRank vector</li>
</ul>
</li>
</ul>
<h2>Analyzing Relational Data</h2>
<p>Database Workloads</p>
<ul>
<li><strong>OLTP</strong>: Online Transaction Processing
<ul>
<li>user facing, real-time, low-latency</li>
<li>Application users want fast applications
<ul>
<li>Typical access patterns</li>
<li>random access</li>
<li>writes on small amounts of data</li>
<li>small amounts of data per query</li>
</ul>
</li>
</ul>
</li>
<li><strong>OLAP</strong>: Online Analytical Processing
<ul>
<li>Business intelligence, data mining</li>
<li>batch workloads, less concurrency</li>
<li>Analysts want to be able to analyze large amounts of data
<ul>
<li>Full table scans</li>
<li>lots of joins</li>
<li>large amounts of data per query</li>
</ul>
</li>
</ul>
</li>
<li>Hard for OLTP and OLAP to exist together
<ul>
<li>poor memory management</li>
<li>conflicting data accessing patterns</li>
<li>variable latency</li>
<li><strong>solution</strong> Build a separate data warehouse</li>
<li>OTLP -&gt; ETL -&gt; OLAP
<ul>
<li>Run ETL on some frequency (ex. every night)</li>
<li><strong>Implication</strong>: Analysts are working on old data</li>
<li>Extract</li>
<li>Transform
<ul>
<li>Data cleaning and integrity checking</li>
<li>schema conversion</li>
<li>field transformations</li>
</ul>
</li>
<li>Load</li>
</ul>
</li>
</ul>
</li>
<li><strong>OLAP cubes</strong>: Join tables together, perform operations
<ul>
<li>slice and dice
<ul>
<li>slice the cube into areas that your interested in</li>
<li>ex. take a time slice, only care about the last month</li>
</ul>
</li>
<li>roll-up /down
<ul>
<li>Dimensions have heirarchial structure: time is composed of hours / days / years</li>
<li>Specific products categories / sub-categories</li>
<li>drill down: Stores in Ontario -&gt; Stores in Southern-Ontario -&gt; Stores in Waterloo</li>
</ul>
</li>
<li>pivot
<ul>
<li>Rotate the cube</li>
<li>Sales figures for products and stores, rotate to see changes by month</li>
<li>Perform aggregation along some different axis</li>
</ul>
</li>
<li>Cube materialization
<ul>
<li>lots of joins, group-bys and aggregations</li>
<li>pre-compute parts of the cube</li>
<li>trade-off between time and space</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>What do you do with the data?</p>
<ul>
<li>Report Generation</li>
<li>Dashboards</li>
<li>Ad-hoc analyses
<ul>
<li>Descriptive: What happened / what is happening</li>
<li>Predictive: predict future, unknown events</li>
</ul>
</li>
<li>Data products</li>
</ul>
<p>Why databases (known unknowns)</p>
<ul>
<li>great for structured data, can use schemas!</li>
<li>data is clean</li>
<li>know what queries you want to run on it</li>
</ul>
<p>Why Not databases (unknown unknowns)</p>
<ul>
<li>little to no structure in data</li>
<li>data is messy and noisy</li>
<li>don't know what your looking for</li>
<li>behavioural data</li>
</ul>
<p>Advantages of Hadoop (dataflow language)</p>
<ul>
<li>don't need to know a schema ahead of time</li>
<li>raw scans are the most common operations</li>
<li>many analyses are better formatted imperitevely</li>
<li>much faster data injest rate</li>
</ul>
<p>Utilize both to gain the most benefits and flexibility:
<strong>Data Lake</strong>: Store of raw data
<strong>Data Warehouse</strong>: Structured data</p>
<p>Future: <strong>HTAP</strong>: Hybrid Transactional Analytical Processing</p>
<ul>
<li>avoid the ETL process</li>
<li>Let's analysts work with realtime data</li>
</ul>
<h3>MapReduce algorithms for relational data</h3>
<ul>
<li>Projection (<img src="data/final_notes/latex-a21ed13c-8f49-4d80-b06d-e4af9af0900c.png" alt="latex-a21ed13c-8f49-4d80-b06d-e4af9af0900c">)
<ul>
<li>Select particular fields for each tuple</li>
<li>Need to keep track of field positions after projections</li>
</ul>
</li>
<li>Selection (<img src="data/final_notes/latex-a387220f-89d4-4121-a0a1-e9594897b6aa.png" alt="latex-a387220f-89d4-4121-a0a1-e9594897b6aa">)
<ul>
<li>Process each tuple, emit those that meet some criteria</li>
<li>Pipeline with projections</li>
<li>Performance limited by HDFS throughput
<ul>
<li>speed of encoding/decoding tuples is important</li>
<li>take advantage of compression</li>
</ul>
</li>
</ul>
</li>
<li>Groupby ... Aggregation
<ul>
<li>Some aggregation function</li>
<li>Map over dataset, emit tuples, key by the group by attribute. <strong>Framework handles the actual grouping</strong></li>
<li>Compute the aggregation function in the reducer</li>
<li>Optimize with combiners, IMC</li>
</ul>
</li>
<li>Joins
<ul>
<li>Reduce side (repartition, shuffle join)
<ul>
<li>map over both datasets</li>
<li>emit tuple value with join key as the intermediate key</li>
<li>framework brings together</li>
<li>perform join in reducer</li>
</ul>
</li>
<li>Map Side (sort-merge)
<ul>
<li>both datasets must be copartitioned (partitioned in the same way), allows for parallelization</li>
<li>Map over one dataset, read from the other corresponding partition</li>
<li>no reducer needed</li>
</ul>
</li>
<li>Hash (broadcast, replicated)
<ul>
<li>Load one dataset into memory, keyed by the join key</li>
<li>Read the other dataset, probe for the join key
<ul>
<li>Store R in DistributedCache, progate to all workers</li>
<li>no reducers necessary</li>
</ul>
</li>
<li>Need <code>R &lt;&lt; S</code> and R must fit into memory</li>
<li>variants
<ul>
<li>R and S are co-partitioned: only need to build hash maps for the corresponding partition</li>
<li>striped: If R doesn't fit into memory
<ul>
<li>Divide R into n groups, such that each fit into memory</li>
<li>Perform a hash join for all n</li>
<li>Union the results together</li>
</ul>
</li>
<li>Use a global key-value store (Memcached), prob the key-value store</li>
</ul>
</li>
</ul>
</li>
<li>Preferences (Most, ... ,Least): Hash &gt; Map &gt; Reduce</li>
</ul>
</li>
</ul>
<p>Limitations of Joining</p>
<table>
<thead>
<tr>
<th>Hash</th>
<th>Map</th>
<th>Reduce</th>
</tr>
</thead>
<tbody>
<tr>
<td>Memory</td>
<td>Sort order and partitioning</td>
<td>General purpose</td>
</tr>
</tbody>
</table>
<p>Running SQL-on-Hadoop</p>
<ol>
<li>Build logical plan</li>
<li>optimize logical plan</li>
<li>select physical plan</li>
</ol>
<p>Hadoop Dataware house designs</p>
<ul>
<li>Joins are expensive</li>
<li>Ultimately a time-space trade-off
<ul>
<li>Have more redundant data around in order to save on later joins (save time, waste space)</li>
</ul>
</li>
<li>normalization: factor out redundancy</li>
<li>denormalizations: pre-join to the Facts table in order to save the cost of later joins
<ul>
<li>Denormalize can occur during the ETL process</li>
</ul>
</li>
</ul>
<p>Row vs. column Stores</p>
<ul>
<li>Row store: <code>(a, b, c), (a, b, c), ....</code>
<ul>
<li>Easy to modify a record, in place updates</li>
<li>May need to read un-necessary data during processing</li>
</ul>
</li>
<li>Column store: <code>(a, a, a, a..., ), (b, b, b, ...), (c, c, c, ...)</code>
<ul>
<li>only read the necessary data when processing</li>
<li>tuple writes require multiple operations</li>
<li>tuple updates are complex</li>
<li>better compression, read efficiency, vectorized execution, compiled queries</li>
</ul>
</li>
</ul>
<p>Physical Representation</p>
<ul>
<li>Use binary representations (parqueet, protobuf)</li>
<li>define schemas for binary</li>
<li>Use an index to speed up hadoop, InputSplits on contain blocks that match selection crtieria. Instead of processing everything and thowing away most. Implement at the RecordReader level.</li>
</ul>
<h3>Predictive Analytics</h3>
<ul>
<li>Descriptive Analytics: Describe what is currently happening</li>
<li>Predictive Analytics: Predict future values</li>
<li>Classification: output draw from labels</li>
<li>Data -&gt; Features -&gt; Model -&gt; Optimization</li>
</ul>
<p>Features</p>
<ul>
<li>Dense: most samples contain the features</li>
<li>Sparse: messages that contain specific terms</li>
<li>Gathering labelled data can be a bottleneck
<ul>
<li>crowdsource</li>
<li>Bootstrapping, semi-supervised techniques</li>
<li>Exploiting user behaviour logs: emojis for sentiments</li>
</ul>
</li>
<li>Supervised Binary Classification: restrict output label to binary
<ul>
<li>Extend to multi-class with multiple classifiers
<ul>
<li>1 vs. rest: A or not, B or not, ...</li>
<li>Classifier Cascading: A or Not -&gt; B or Not -&gt; ...
<ul>
<li>Keep going until success</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Minimize some loss function, gradient descent, lower loss over time
<ul>
<li>Gradient descent is first order techniques, use higher order derivatives</li>
</ul>
</li>
<li><strong>Logistic Regression</strong>:
<ul>
<li>Set equal to one: <img src="data/final_notes/latex-a3da373b-fc85-4da9-9ff2-0338aa28075a.png" alt="latex-a3da373b-fc85-4da9-9ff2-0338aa28075a"></li>
<li><img src="data/final_notes/latex-aa4a28f9-6184-4267-b206-4c2d7d2fd4d2.png" alt="latex-aa4a28f9-6184-4267-b206-4c2d7d2fd4d2"></li>
<li><img src="data/final_notes/latex-9ab69383-42c4-48f2-bfaa-b39afcbec0b2.png" alt="latex-9ab69383-42c4-48f2-bfaa-b39afcbec0b2"></li>
<li><img src="data/final_notes/latex-48c502a2-24f9-4029-a31c-4070c50e689b.png" alt="latex-48c502a2-24f9-4029-a31c-4070c50e689b">
<ul>
<li>Minimize the log likelihood</li>
</ul>
</li>
<li><img src="data/final_notes/latex-352c8847-6bd2-4ede-8c1b-33eaba229d53.png" alt="latex-352c8847-6bd2-4ede-8c1b-33eaba229d53">
<ul>
<li>Utilize the gradient to update weights</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><img src="gradient.png" alt="Gradient Descent"></p>
<pre><code class="language-scala">val points = spark.textFile(...).map(parsePoint).persist()
var w = // random initial vector
for (i &lt;- 1 to ITERATIONS) {
  val gradient = points.map{ p =&gt;
    p.x * (1/(1+exp(-p.y*(w dot p.x)))-1)*p.y
  }.reduce((a,b) =&gt; a+b)
  w -= gradient
}
</code></pre>
<p>Batch Learning: Update the model after considering all training instances
Online Learning: Update the model after considering each randomly selected training example.</p>
<ul>
<li>stocastic gradient descent</li>
<li>Randomly shuffle the training samples</li>
<li>Mini-batching as a middle ground</li>
</ul>
<p>Ensemble Learning: Use multiple independent models to make a prediction</p>
<ul>
<li>Train classifiers on distinct partitions of the data</li>
<li>Combining Predictions
<ul>
<li>Majority Voting: Take the class that most models pick</li>
<li>Simple Weighted Voting: Apply some weights to each prediction</li>
<li>Model Averaging</li>
</ul>
</li>
<li>This works as long as the models are independent. If the errors are uncorrelated then the chances of multiple classifiers being wrong is less likely.</li>
</ul>
<p>Metrics</p>
<table>
<thead>
<tr>
<th>Actual</th>
<th>Predicted</th>
<th>Error</th>
</tr>
</thead>
<tbody>
<tr>
<td>Positive</td>
<td>Positive</td>
<td>True Positive</td>
</tr>
<tr>
<td>Negative</td>
<td>Positive</td>
<td>False Positive (Type 1 Error)</td>
</tr>
<tr>
<td>Positive</td>
<td>Negative</td>
<td>False Negative (Type 2 Error)</td>
</tr>
<tr>
<td>Negative</td>
<td>Negative</td>
<td>True Negative</td>
</tr>
</tbody>
</table>
<ul>
<li>Cross-Validation: Train / Test Split, validation set to test the model during training.</li>
</ul>
<p>Similarity:</p>
<ul>
<li>Offline variant: extract all similar pairs of objects from a large collection</li>
<li>Online variant: is this object similar to something Iâ€™ve seen before?</li>
<li><strong>Steps:</strong> Specify distance metric -&gt; compute representation -&gt; Project -&gt; extract</li>
</ul>
<p>Minhash</p>
<ul>
<li>near duplicate detection of webpages, HTML represented as shingles (n-grams)</li>
<li>For all elements, binary vector if in each document, M00 = both 0, M11 = both 1, M01, M10. Jaccard: <img src="data/final_notes/latex-94bcac99-08a4-4ec1-9528-4c4434d781c5.png" alt="latex-94bcac99-08a4-4ec1-9528-4c4434d781c5"></li>
<li>Randomly permute the rows of the matrix, minhash is then the first row with a 1. Then <img src="data/final_notes/latex-44616da8-c28c-4d6b-a83a-d36689d4b705.png" alt="latex-44616da8-c28c-4d6b-a83a-d36689d4b705"></li>
<li><strong>Task</strong>: Discover all pairs with similarity greater than s, <strong>FP</strong>: discovered pairs that have a similarity less than s, <strong>FN</strong>: pairs with similarity greater than s not discovered.</li>
<li><strong>Alg</strong>: Foreach object, compute minhash value, group objects by their hash value, output all pairs within each group. Threshold controls the amount of FPs and FNs.</li>
<li>Use k hash functions, probability of detection is <img src="data/final_notes/latex-8a72882f-67b1-47ff-a840-d2906af20beb.png" alt="latex-8a72882f-67b1-47ff-a840-d2906af20beb">.</li>
<li>n sets k: for each object, compute n sets k minhash, for each set concat k minhash values together, in each set group objects by signature (k minhash values), output all pairs in each group, de-dup pairs. If J(A, B) = s; P(none of the values collide) = <img src="data/final_notes/latex-73edc8db-4371-4a12-9c4e-14581e8470d8.png" alt="latex-73edc8db-4371-4a12-9c4e-14581e8470d8">, P(Detection) = 1 - P(none collide).</li>
</ul>
<p>Batch formulation: Discover all pairs with similarity greater than s.
Online formulation: Given a new page, is it similar to one that we've seen before.
Minhash problems: good for near-duplicate detection, jaccard only, no way to assign weights to features,</p>
<p>Arrange Similar Items into Clusters</p>
<ul>
<li>Offline variant: entire static collection available at once</li>
<li>Online variant: objects incrementally available</li>
<li>forming clusters: high similarity between items in the same cluster, low similarity between items in different clusters.</li>
<li><strong>HOW</strong>: Distance metric (jaccard, euclid) -&gt; compute representation (shingling, tf.idf) -&gt; apply clustering alg</li>
<li>Distance = 1 - similarity</li>
<li>Hierarchical Agglomerative Clustering: start with each item in it's own cluster, until there is only 1 cluster, find the two most similar clusters, merge. The history of merges form the heirarchy.</li>
<li>What is the similarity between two clusters:
<ul>
<li>single link: similarity of 2 most similar members
<ul>
<li>Can result in long and thin clusters</li>
</ul>
</li>
<li>complete link: similarity of 2 least similar members
<ul>
<li>can result in round clusters</li>
</ul>
</li>
<li>Group average: average similarity between members</li>
</ul>
</li>
<li>K-MEANS: Select k random instances as centroids
<ul>
<li>iterate: assign each instance to it's closest centroid, update centroid based on assigned instance.</li>
</ul>
</li>
</ul>
<h3>Mutable State</h3>
<p>We want to keep track of mutable state in a scalable mannar. state organized in terms of logical records, state is unlikely to fit on a single machine, distributed.</p>
<p>Why not use RDBMS:</p>
<ul>
<li>Everything must be designed up front, annoying to change</li>
<li>ACID properties can be expensive:
<ul>
<li>Atomic: Transactions are all or nothing</li>
<li>consistent: A transaction will bring the database from one valid state to another.</li>
<li>isolated: Ensures that transactions executed concurrently would yield the same results had they been executed sequentially.</li>
<li>Durability: Once a transaction has been committed, it will remain so.</li>
</ul>
</li>
<li>potentially High cost</li>
</ul>
<p>Can gain extra features by trading away other properties (NoSQL)</p>
<ul>
<li>Types: Key-value store, column ordiented database, document stores, graph database.</li>
<li>Ideas:
<ul>
<li>Partitioning (Sharding): Need to keep track of the partitions
<ul>
<li>increase scalability, decrease latency</li>
</ul>
</li>
<li>Replication: Need to remain consistent
<ul>
<li>To incrase robustness (availability), increase throughput</li>
</ul>
</li>
<li>Caching: Consistency
<ul>
<li>reduce latency</li>
</ul>
</li>
</ul>
</li>
<li><strong>Key-value stores</strong>: store associations between keys and values. primitive keys, values are blobs. Ops: <code>get</code>: value for key, <code>put</code>: set value for key. <strong>Consistency Model</strong>: Atomic puts, no cross-key operations
<ul>
<li>Non-persistent: just a big in-memory hash table (Redis, Memcached)</li>
<li>Persistent: wrapper around a traditional RDBMS</li>
<li>Doesn't fit on a single machine: <strong>Partition</strong>. Hash the keys and hash the machines. Machines hold pointers the next and prev, finger tables (+2, +4, ..). Route requests in <img src="data/final_notes/latex-b14cb4cb-c211-40fc-acdf-1f95473ffa6a.png" alt="latex-b14cb4cb-c211-40fc-acdf-1f95473ffa6a"></li>
</ul>
</li>
</ul>
<p>BigTable (HBase)</p>
<ul>
<li>Table is sparse, distributed, persistent, multi-dimensinoal sorted map. Indexed by <code>(row:string, column:string, time:int64) -&gt; uniterpreted byte array</code></li>
<li>Lookup, inserts, deletes: single row transactions only</li>
<li>rows: sorted lexicographically, efficient row scans. columns grouped into families <code>family:qualifier</code>, provide locality hints, unbounded number of columns. All just key value pairs.</li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>In Memory</th>
<th>On Disk</th>
</tr>
</thead>
<tbody>
<tr>
<td>Mutability</td>
<td>Easy</td>
<td>Hard</td>
</tr>
<tr>
<td>Size</td>
<td>Small</td>
<td>Big</td>
</tr>
</tbody>
</table>
<p>Log-structured Merge Trees</p>
<ul>
<li>In memory store, when full: flush to disk</li>
<li>On disk Stores are immutable, indexed, persistent k-v pairs.</li>
<li>Write Ahead Log for persistance</li>
</ul>
<p>Distributed Version: Use HDFS</p>
<ul>
<li>Replication for free</li>
<li>Persistent, ordered immutable map from keys to values</li>
<li><strong>operations</strong>: Look-up value for key, iterate over key/value pairs in range.</li>
</ul>
<p>Trade-offs
Partitioning: Need distributed transactions</p>
<ul>
<li>Use a two-phase commit (coordinator, subordinates), use WAL at each node, <strong>cons</strong>: blocking and slow, problems if coordinator dies.
Replication: Need replica coherence protocol
Caching: Need caching coherence protocol</li>
</ul>
<p><strong>CAP Theorem</strong></p>
<ul>
<li>Consistency, Availability, Partition Tolerence.</li>
<li>Pick 2.</li>
<li>CA: Constistency + Availability. Parallel databases that use 2PC.</li>
<li>Availiability + Partition Tolerence, Web CAching, DNS</li>
</ul>
<p><strong>PACELC</strong></p>
<ul>
<li>PAC: If there's a partition, choose A or C.</li>
<li>ELC: Otherwise, do we choose LAtency or Consistency</li>
<li>The choice the application dependent.</li>
</ul>
<h3>Graphs Redux</h3>
<ul>
<li>Can be smarter about partitioning graphs, use underlying graph structure
<ul>
<li>webpages on the same domain are likely to be connected, domain reversed urls <code>cnn.com -&gt; com.cnn</code>. In social networks, domain characteristics</li>
</ul>
</li>
</ul>
<h3>Real-Time Analytics</h3>
<p>Recall: Data scientists typically are working with old data.</p>
<ul>
<li>implication: Results an be hours behind reality, when things move fast this can be bad (delayed query recommendations)</li>
<li>Real-time: low-latency</li>
<li>Online: Contrasted with batch processing</li>
<li>Stream: Nature of the data that arrives
<ul>
<li>Data stream: an ordered, continuously arriving sequence of tuples. May not be possible to store everything, may not be possible to examine everything.</li>
</ul>
</li>
</ul>
<p>Windowing:</p>
<ul>
<li>restrict the processing scope</li>
<li>based on ordering (time), item counts, explicit markers (punctuations)</li>
<li>Sliding window: increment by amount. Tumbling Window: Each window is descrete (no overlap)</li>
<li>Punctuations: Application inserted end of processing, Pro: Application controlled window semantics, cons: unpredictable window size (too big / small).</li>
</ul>
<p>Stream Processing Challenges</p>
<ul>
<li>Inherent Challenges: LAtency requirements, space bounds</li>
<li>System Challenges:
<ul>
<li>Bursty behaviour and load balancing:</li>
<li>Out of order message delivery, non-determinism</li>
<li>Consistency semantics (at most once, exactly once, at least once)</li>
</ul>
</li>
</ul>
<p>Producer / Consumer</p>
<ul>
<li>Producers push data to consumers (via callbacks)</li>
<li>Consumer pulls data from producer (via polling)</li>
<li>Insert a broker (Kafka) in the middle, Producers send to broker, Consumers get data from broker</li>
</ul>
<p>Storm / Heron</p>
<ul>
<li>A job topology is a computational graph, directed edges indicate communication between edges</li>
<li>Processing semantics: At most once (without acknowledgments), at least once (with acks)</li>
<li>Spouts: emit tuples. Bolts: Processing tuples.</li>
</ul>
<p>Spark Streaming: Descretized RDDs</p>
<ul>
<li>Problem: no differentiation between the event-time and processing time, when the event actually occurred and when the tuple is processed</li>
</ul>
<p><strong>Watermark</strong>: System's notion when all data in a window is expected to arrive. Default trigger fires at the watermark.</p>
<p>Dealing with Streaming Problems</p>
<ul>
<li>Throw away data: <strong>Sampling</strong>
<ul>
<li>Reservoir Sampling: Select s elements from a stream of size n with uniform probability. Store the first s elements, for every k-th element: keep with probability <img src="data/final_notes/latex-3d0996ff-4cba-49d3-b067-5609f6e981a9.png" alt="latex-3d0996ff-4cba-49d3-b067-5609f6e981a9">, randomly discard an existing sample.</li>
</ul>
</li>
<li>Approximations (hashing)
<ul>
<li>Set Cardinality: HashSet, HyperLogLog Counter
<ul>
<li>hash each item and examine the hash code, keep track of some unique pattern (number of leading zeros). Use multiple hash functions and do it lots of times.</li>
</ul>
</li>
<li>Set Membership: HashSet, Bloom Filter
<ul>
<li><code>put(x)</code>: insert x into set, <code>contains(x)</code>: yes if x is a member of a set.</li>
<li>m-bit vector, k hash functions. put(x): run x through each hash function, set the corresponding bit to 1.</li>
<li>contains: run the item through the hash functions, check if all of the bits are 1. False positives possible, no false negatives.</li>
</ul>
</li>
<li>Frequency Estimation: HashMap, Count Min Sketches
<ul>
<li><code>put(x)</code>: increment the count of x by 1. <code>get(x)</code> return the frequency of x.</li>
<li><code>m x k</code> array of counters, k hash functions. put run item though each hash function, increment the corresponding count. get: run item through each hash function, return the minimum count.</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
